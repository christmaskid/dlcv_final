2024-12-12 18:40:13,223 INFO: Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

2024-12-12 18:40:13,223 INFO: 
  name: watercolor
  manual_seed: 0
  mixed_precision: fp16
  gradient_accumulation_steps: 1
  datasets:[
    train:[
      name: LoraDataset
      concept_list: /content/Data/jsons/watercolor.json
      use_caption: True
      use_mask: True
      instance_transform: [{'type': 'HumanResizeCropFinalV3', 'size': 512, 'crop_p': 0.5}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': [0.5], 'std': [0.5]}, {'type': 'ShuffleCaption', 'keep_token_num': 1}, {'type': 'EnhanceText', 'enhance_type': 'object'}]
      replace_mapping:[
        <TOK>: <watercolor_1> <watercolor_2>
      ]
      batch_size_per_gpu: 2
      dataset_enlarge_ratio: 500
    ]
    val_vis:[
      name: PromptDataset
      prompts: /content/dlcv_final/Concept-Conductor/val.txt
      num_samples_per_prompt: 1
      latent_size: [4, 64, 64]
      replace_mapping:[
        <TOK>: <watercolor_1> <watercolor_2>
      ]
      batch_size_per_gpu: 4
    ]
  ]
  models:[
    pretrained_path: /content/dlcv_final/Concept-Conductor/experiments/pretrained_models/stable-diffusion-v1-5/
    enable_edlora: True
    finetune_cfg:[
      text_embedding:[
        enable_tuning: True
        lr: 0.001
      ]
      text_encoder:[
        enable_tuning: True
        lora_cfg:[
          rank: 4
          alpha: 1
          where: CLIPSdpaAttention
        ]
        lr: 1e-05
      ]
      unet:[
        enable_tuning: True
        lora_cfg:[
          rank: 4
          alpha: 1
          where: Attention
        ]
        lr: 0.0001
      ]
    ]
    new_concept_token: <watercolor_1>+<watercolor_2>
    noise_offset: 0.01
    initializer_token: <rand-0.013>+watercolor
    attn_reg_weight: 0.01
    reg_full_identity: False
    use_mask_loss: True
    gradient_checkpoint: False
    enable_xformers: True
  ]
  path:[
    pretrain_network: None
    experiments_root: /content/dlcv_final/Concept-Conductor/experiments/watercolor
    models: /content/dlcv_final/Concept-Conductor/experiments/watercolor/models
    log: /content/dlcv_final/Concept-Conductor/experiments/watercolor
    visualization: /content/dlcv_final/Concept-Conductor/experiments/watercolor/visualization
  ]
  train:[
    optim_g:[
      type: AdamW
      lr: 0.0
      weight_decay: 0.01
      betas: [0.9, 0.999]
    ]
    unet_kv_drop_rate: 0
    scheduler: linear
    emb_norm_threshold: 0.55
  ]
  val:[
    val_during_save: True
    compose_visualize: True
    alpha_list: [0, 0.7, 1.0]
    sample:[
      num_inference_steps: 50
      guidance_scale: 7.5
    ]
  ]
  logger:[
    print_freq: 10
    save_checkpoint_freq: 10000.0
  ]
  is_train: True

2024-12-12 18:40:14,530 INFO: <watercolor_1> (49408-49423) is random initialized by: <rand-0.013>
2024-12-12 18:40:15,233 INFO: <watercolor_2> (49424-49439) is random initialized by existing token (watercolor): 14211
2024-12-12 18:40:15,238 INFO: optimizing embedding using lr: 0.001
2024-12-12 18:40:15,250 INFO: optimizing text_encoder (48 LoRAs), using lr: 1e-05
2024-12-12 18:40:15,284 INFO: optimizing unet (128 LoRAs), using lr: 0.0001
2024-12-12 18:40:16,815 INFO: ***** Running training *****
2024-12-12 18:40:16,816 INFO:   Num examples = 1500
2024-12-12 18:40:16,816 INFO:   Instantaneous batch size per device = 2
2024-12-12 18:40:16,816 INFO:   Total train batch size (w. parallel, distributed & accumulation) = 2
2024-12-12 18:40:16,816 INFO:   Total optimization steps = 750.0
2024-12-12 18:40:29,427 INFO: [water..][Iter:      10, lr:(9.867e-04,9.867e-06,9.867e-05,)] [eta: 0:14:07] loss: 4.5789e-02 Norm_mean: 4.0286e-01 
2024-12-12 18:40:41,112 INFO: [water..][Iter:      20, lr:(9.733e-04,9.733e-06,9.733e-05,)] [eta: 0:14:03] loss: 3.0439e-01 Norm_mean: 4.1786e-01 
2024-12-12 18:40:53,067 INFO: [water..][Iter:      30, lr:(9.600e-04,9.600e-06,9.600e-05,)] [eta: 0:14:00] loss: 2.8391e-01 Norm_mean: 4.2851e-01 
2024-12-12 18:41:04,825 INFO: [water..][Iter:      40, lr:(9.467e-04,9.467e-06,9.467e-05,)] [eta: 0:13:50] loss: 8.4918e-01 Norm_mean: 4.3635e-01 
2024-12-12 18:41:16,165 INFO: [water..][Iter:      50, lr:(9.333e-04,9.333e-06,9.333e-05,)] [eta: 0:13:33] loss: 6.4576e-02 Norm_mean: 4.4316e-01 
2024-12-12 18:41:27,686 INFO: [water..][Iter:      60, lr:(9.200e-04,9.200e-06,9.200e-05,)] [eta: 0:13:20] loss: 6.8672e-01 Norm_mean: 4.4914e-01 
2024-12-12 18:41:39,170 INFO: [water..][Iter:      70, lr:(9.067e-04,9.067e-06,9.067e-05,)] [eta: 0:13:07] loss: 7.8147e-01 Norm_mean: 4.5441e-01 
2024-12-12 18:41:50,731 INFO: [water..][Iter:      80, lr:(8.933e-04,8.933e-06,8.933e-05,)] [eta: 0:12:55] loss: 2.7475e-01 Norm_mean: 4.5907e-01 
2024-12-12 18:42:02,163 INFO: [water..][Iter:      90, lr:(8.800e-04,8.800e-06,8.800e-05,)] [eta: 0:12:42] loss: 2.0221e-02 Norm_mean: 4.6369e-01 
2024-12-12 18:42:13,913 INFO: [water..][Iter:     100, lr:(8.667e-04,8.667e-06,8.667e-05,)] [eta: 0:12:32] loss: 3.2857e-01 Norm_mean: 4.6851e-01 
2024-12-12 18:42:25,241 INFO: [water..][Iter:     110, lr:(8.533e-04,8.533e-06,8.533e-05,)] [eta: 0:12:19] loss: 5.5476e-01 Norm_mean: 4.7386e-01 
2024-12-12 18:42:36,685 INFO: [water..][Iter:     120, lr:(8.400e-04,8.400e-06,8.400e-05,)] [eta: 0:12:07] loss: 7.9491e-01 Norm_mean: 4.7817e-01 
2024-12-12 18:42:48,304 INFO: [water..][Iter:     130, lr:(8.267e-04,8.267e-06,8.267e-05,)] [eta: 0:11:55] loss: 4.5459e-01 Norm_mean: 4.8266e-01 
2024-12-12 18:42:59,605 INFO: [water..][Iter:     140, lr:(8.133e-04,8.133e-06,8.133e-05,)] [eta: 0:11:43] loss: 1.6691e+00 Norm_mean: 4.8712e-01 
2024-12-12 18:43:11,140 INFO: [water..][Iter:     150, lr:(8.000e-04,8.000e-06,8.000e-05,)] [eta: 0:11:31] loss: 1.7208e-01 Norm_mean: 4.9136e-01 
2024-12-12 18:43:22,603 INFO: [water..][Iter:     160, lr:(7.867e-04,7.867e-06,7.867e-05,)] [eta: 0:11:19] loss: 1.1271e+00 Norm_mean: 4.9522e-01 
2024-12-12 18:43:34,272 INFO: [water..][Iter:     170, lr:(7.733e-04,7.733e-06,7.733e-05,)] [eta: 0:11:08] loss: 1.0327e-01 Norm_mean: 4.9873e-01 
2024-12-12 18:43:45,738 INFO: [water..][Iter:     180, lr:(7.600e-04,7.600e-06,7.600e-05,)] [eta: 0:10:56] loss: 6.4712e-02 Norm_mean: 5.0264e-01 
2024-12-12 18:43:57,231 INFO: [water..][Iter:     190, lr:(7.467e-04,7.467e-06,7.467e-05,)] [eta: 0:10:45] loss: 1.0038e-01 Norm_mean: 5.0752e-01 
2024-12-12 18:44:08,907 INFO: [water..][Iter:     200, lr:(7.333e-04,7.333e-06,7.333e-05,)] [eta: 0:10:33] loss: 7.4794e-01 Norm_mean: 5.1265e-01 
2024-12-12 18:44:20,411 INFO: [water..][Iter:     210, lr:(7.200e-04,7.200e-06,7.200e-05,)] [eta: 0:10:22] loss: 7.3929e-01 Norm_mean: 5.1730e-01 
2024-12-12 18:44:31,875 INFO: [water..][Iter:     220, lr:(7.067e-04,7.067e-06,7.067e-05,)] [eta: 0:10:10] loss: 1.2584e+00 Norm_mean: 5.2105e-01 
2024-12-12 18:44:43,796 INFO: [water..][Iter:     230, lr:(6.933e-04,6.933e-06,6.933e-05,)] [eta: 0:09:59] loss: 3.2473e-01 Norm_mean: 5.2437e-01 
2024-12-12 18:44:55,362 INFO: [water..][Iter:     240, lr:(6.800e-04,6.800e-06,6.800e-05,)] [eta: 0:09:48] loss: 1.3952e+00 Norm_mean: 5.2729e-01 
2024-12-12 18:45:06,914 INFO: [water..][Iter:     250, lr:(6.667e-04,6.667e-06,6.667e-05,)] [eta: 0:09:36] loss: 1.3500e+00 Norm_mean: 5.3027e-01 
2024-12-12 18:45:18,531 INFO: [water..][Iter:     260, lr:(6.533e-04,6.533e-06,6.533e-05,)] [eta: 0:09:25] loss: 3.5504e-01 Norm_mean: 5.3309e-01 
2024-12-12 18:45:30,110 INFO: [water..][Iter:     270, lr:(6.400e-04,6.400e-06,6.400e-05,)] [eta: 0:09:13] loss: 6.3227e-01 Norm_mean: 5.3541e-01 
2024-12-12 18:45:41,755 INFO: [water..][Iter:     280, lr:(6.267e-04,6.267e-06,6.267e-05,)] [eta: 0:09:02] loss: 6.3957e-01 Norm_mean: 5.3823e-01 
2024-12-12 18:45:53,387 INFO: [water..][Iter:     290, lr:(6.133e-04,6.133e-06,6.133e-05,)] [eta: 0:08:50] loss: 7.0265e-01 Norm_mean: 5.4090e-01 
2024-12-12 18:46:04,896 INFO: [water..][Iter:     300, lr:(6.000e-04,6.000e-06,6.000e-05,)] [eta: 0:08:39] loss: 5.1243e-01 Norm_mean: 5.4309e-01 
2024-12-12 18:46:16,490 INFO: [water..][Iter:     310, lr:(5.867e-04,5.867e-06,5.867e-05,)] [eta: 0:08:27] loss: 1.1057e-01 Norm_mean: 5.4520e-01 
2024-12-12 18:46:28,024 INFO: [water..][Iter:     320, lr:(5.733e-04,5.733e-06,5.733e-05,)] [eta: 0:08:16] loss: 4.9343e-01 Norm_mean: 5.4705e-01 
2024-12-12 18:46:39,712 INFO: [water..][Iter:     330, lr:(5.600e-04,5.600e-06,5.600e-05,)] [eta: 0:08:04] loss: 2.2147e-01 Norm_mean: 5.4887e-01 
2024-12-12 18:46:51,281 INFO: [water..][Iter:     340, lr:(5.467e-04,5.467e-06,5.467e-05,)] [eta: 0:07:53] loss: 8.7908e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:47:02,708 INFO: [water..][Iter:     350, lr:(5.333e-04,5.333e-06,5.333e-05,)] [eta: 0:07:41] loss: 5.6139e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:47:14,222 INFO: [water..][Iter:     360, lr:(5.200e-04,5.200e-06,5.200e-05,)] [eta: 0:07:29] loss: 6.2808e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:47:25,719 INFO: [water..][Iter:     370, lr:(5.067e-04,5.067e-06,5.067e-05,)] [eta: 0:07:18] loss: 6.6731e-02 Norm_mean: 5.5003e-01 
2024-12-12 18:47:37,394 INFO: [water..][Iter:     380, lr:(4.933e-04,4.933e-06,4.933e-05,)] [eta: 0:07:06] loss: 2.6546e-02 Norm_mean: 5.5003e-01 
2024-12-12 18:47:49,156 INFO: [water..][Iter:     390, lr:(4.800e-04,4.800e-06,4.800e-05,)] [eta: 0:06:55] loss: 9.4114e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:48:00,645 INFO: [water..][Iter:     400, lr:(4.667e-04,4.667e-06,4.667e-05,)] [eta: 0:06:43] loss: 1.6628e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:48:12,250 INFO: [water..][Iter:     410, lr:(4.533e-04,4.533e-06,4.533e-05,)] [eta: 0:06:32] loss: 7.1118e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:48:23,677 INFO: [water..][Iter:     420, lr:(4.400e-04,4.400e-06,4.400e-05,)] [eta: 0:06:20] loss: 5.7198e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:48:35,184 INFO: [water..][Iter:     430, lr:(4.267e-04,4.267e-06,4.267e-05,)] [eta: 0:06:08] loss: 5.6012e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:48:46,762 INFO: [water..][Iter:     440, lr:(4.133e-04,4.133e-06,4.133e-05,)] [eta: 0:05:57] loss: 1.9279e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:48:58,381 INFO: [water..][Iter:     450, lr:(4.000e-04,4.000e-06,4.000e-05,)] [eta: 0:05:45] loss: 3.9210e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:49:09,828 INFO: [water..][Iter:     460, lr:(3.867e-04,3.867e-06,3.867e-05,)] [eta: 0:05:34] loss: 2.2563e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:49:21,363 INFO: [water..][Iter:     470, lr:(3.733e-04,3.733e-06,3.733e-05,)] [eta: 0:05:22] loss: 3.8843e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:49:32,947 INFO: [water..][Iter:     480, lr:(3.600e-04,3.600e-06,3.600e-05,)] [eta: 0:05:11] loss: 5.1753e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:49:44,421 INFO: [water..][Iter:     490, lr:(3.467e-04,3.467e-06,3.467e-05,)] [eta: 0:04:59] loss: 4.6359e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:49:56,082 INFO: [water..][Iter:     500, lr:(3.333e-04,3.333e-06,3.333e-05,)] [eta: 0:04:47] loss: 7.8431e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:50:07,719 INFO: [water..][Iter:     510, lr:(3.200e-04,3.200e-06,3.200e-05,)] [eta: 0:04:36] loss: 9.1828e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:50:19,428 INFO: [water..][Iter:     520, lr:(3.067e-04,3.067e-06,3.067e-05,)] [eta: 0:04:24] loss: 1.6352e+00 Norm_mean: 5.5003e-01 
2024-12-12 18:50:31,192 INFO: [water..][Iter:     530, lr:(2.933e-04,2.933e-06,2.933e-05,)] [eta: 0:04:13] loss: 2.9418e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:50:42,764 INFO: [water..][Iter:     540, lr:(2.800e-04,2.800e-06,2.800e-05,)] [eta: 0:04:01] loss: 8.7196e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:50:54,323 INFO: [water..][Iter:     550, lr:(2.667e-04,2.667e-06,2.667e-05,)] [eta: 0:03:50] loss: 8.5858e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:51:05,867 INFO: [water..][Iter:     560, lr:(2.533e-04,2.533e-06,2.533e-05,)] [eta: 0:03:38] loss: 7.6832e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:51:17,275 INFO: [water..][Iter:     570, lr:(2.400e-04,2.400e-06,2.400e-05,)] [eta: 0:03:27] loss: 1.8315e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:51:28,767 INFO: [water..][Iter:     580, lr:(2.267e-04,2.267e-06,2.267e-05,)] [eta: 0:03:15] loss: 2.9396e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:51:40,380 INFO: [water..][Iter:     590, lr:(2.133e-04,2.133e-06,2.133e-05,)] [eta: 0:03:03] loss: 5.6170e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:51:51,904 INFO: [water..][Iter:     600, lr:(2.000e-04,2.000e-06,2.000e-05,)] [eta: 0:02:52] loss: 9.2294e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:52:03,312 INFO: [water..][Iter:     610, lr:(1.867e-04,1.867e-06,1.867e-05,)] [eta: 0:02:40] loss: 2.3926e-02 Norm_mean: 5.5003e-01 
2024-12-12 18:52:14,762 INFO: [water..][Iter:     620, lr:(1.733e-04,1.733e-06,1.733e-05,)] [eta: 0:02:29] loss: 8.8862e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:52:26,470 INFO: [water..][Iter:     630, lr:(1.600e-04,1.600e-06,1.600e-05,)] [eta: 0:02:17] loss: 8.2976e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:52:38,130 INFO: [water..][Iter:     640, lr:(1.467e-04,1.467e-06,1.467e-05,)] [eta: 0:02:06] loss: 3.8296e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:52:49,721 INFO: [water..][Iter:     650, lr:(1.333e-04,1.333e-06,1.333e-05,)] [eta: 0:01:54] loss: 1.0895e+00 Norm_mean: 5.5003e-01 
2024-12-12 18:53:01,284 INFO: [water..][Iter:     660, lr:(1.200e-04,1.200e-06,1.200e-05,)] [eta: 0:01:42] loss: 3.0927e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:53:12,901 INFO: [water..][Iter:     670, lr:(1.067e-04,1.067e-06,1.067e-05,)] [eta: 0:01:31] loss: 5.4151e-02 Norm_mean: 5.5003e-01 
2024-12-12 18:53:24,464 INFO: [water..][Iter:     680, lr:(9.333e-05,9.333e-07,9.333e-06,)] [eta: 0:01:19] loss: 1.3171e+00 Norm_mean: 5.5003e-01 
2024-12-12 18:53:36,058 INFO: [water..][Iter:     690, lr:(8.000e-05,8.000e-07,8.000e-06,)] [eta: 0:01:08] loss: 3.2644e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:53:47,646 INFO: [water..][Iter:     700, lr:(6.667e-05,6.667e-07,6.667e-06,)] [eta: 0:00:56] loss: 2.1985e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:53:59,362 INFO: [water..][Iter:     710, lr:(5.333e-05,5.333e-07,5.333e-06,)] [eta: 0:00:45] loss: 1.1267e+00 Norm_mean: 5.5003e-01 
2024-12-12 18:54:11,154 INFO: [water..][Iter:     720, lr:(4.000e-05,4.000e-07,4.000e-06,)] [eta: 0:00:33] loss: 4.8374e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:54:22,696 INFO: [water..][Iter:     730, lr:(2.667e-05,2.667e-07,2.667e-06,)] [eta: 0:00:21] loss: 5.5207e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:54:34,414 INFO: [water..][Iter:     740, lr:(1.333e-05,1.333e-07,1.333e-06,)] [eta: 0:00:10] loss: 2.1613e-01 Norm_mean: 5.5003e-01 
2024-12-12 18:54:46,068 INFO: [water..][Iter:     750, lr:(0.000e+00,0.000e+00,0.000e+00,)] [eta: -1 day, 23:59:59] loss: 7.5395e-02 Norm_mean: 5.5003e-01 
2024-12-12 18:54:46,099 INFO: Save state to /content/dlcv_final/Concept-Conductor/experiments/watercolor/models/edlora_model-latest.pth
2024-12-12 18:54:46,100 INFO: Start validation /content/dlcv_final/Concept-Conductor/experiments/watercolor/models/edlora_model-latest.pth:
